{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSeq2SeqLM, \n    Trainer, \n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\nimport warnings\nwarnings.filterwarnings('ignore')\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Kaggle-specific settings\nimport gc  # Garbage collector for memory management\ntorch.cuda.empty_cache()  # Clear GPU cache\nprint(\"üöÄ Starting Text Summarization Project on Kaggle\")\nprint(\"=\"*60)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-19T13:56:39.800014Z","iopub.execute_input":"2025-08-19T13:56:39.800330Z","iopub.status.idle":"2025-08-19T13:56:39.811225Z","shell.execute_reply.started":"2025-08-19T13:56:39.800301Z","shell.execute_reply":"2025-08-19T13:56:39.806631Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting Text Summarization Project on Kaggle\n============================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Check if GPU is available","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    device = torch.device('cpu')\n    print(\"‚ö†Ô∏è  Using CPU (this will be slower)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T13:56:43.866775Z","iopub.execute_input":"2025-08-19T13:56:43.867052Z","iopub.status.idle":"2025-08-19T13:56:43.878934Z","shell.execute_reply.started":"2025-08-19T13:56:43.867028Z","shell.execute_reply":"2025-08-19T13:56:43.873456Z"}},"outputs":[{"name":"stdout","text":"‚ö†Ô∏è  Using CPU (this will be slower)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Step 2: Load dataset with error handling\n","metadata":{}},{"cell_type":"code","source":"print(f\"\\nüìö Loading CNN/DailyMail dataset...\")\ntry:\n    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n    print(f\"‚úÖ Dataset loaded successfully!\")\n    print(f\"   - Train: {len(dataset['train']):,} examples\")\n    print(f\"   - Validation: {len(dataset['validation']):,} examples\") \n    print(f\"   - Test: {len(dataset['test']):,} examples\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading dataset: {e}\")\n    print(\"üí° Try running: !pip install datasets\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T13:56:44.878503Z","iopub.execute_input":"2025-08-19T13:56:44.878755Z","iopub.status.idle":"2025-08-19T13:56:59.227812Z","shell.execute_reply.started":"2025-08-19T13:56:44.878732Z","shell.execute_reply":"2025-08-19T13:56:59.223674Z"}},"outputs":[{"name":"stdout","text":"\nüìö Loading CNN/DailyMail dataset...\n","output_type":"stream"},{"name":"stderr","text":"Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 287113/287113 [00:04<00:00, 69321.17 examples/s]\nGenerating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13368/13368 [00:00<00:00, 65774.94 examples/s]\nGenerating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11490/11490 [00:00<00:00, 66128.89 examples/s]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Dataset loaded successfully!\n   - Train: 287,113 examples\n   - Validation: 13,368 examples\n   - Test: 11,490 examples\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":" #Preview one example\nprint(f\"\\nüìñ Sample article:\")\nexample = dataset['train'][0]\nprint(f\"Article preview: {example['article'][:200]}...\")\nprint(f\"Summary: {example['highlights']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T13:56:59.230014Z","iopub.execute_input":"2025-08-19T13:56:59.230232Z","iopub.status.idle":"2025-08-19T13:56:59.243466Z","shell.execute_reply.started":"2025-08-19T13:56:59.230211Z","shell.execute_reply":"2025-08-19T13:56:59.236813Z"}},"outputs":[{"name":"stdout","text":"\nüìñ Sample article:\nArticle preview: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported ¬£20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on ...\nSummary: Harry Potter star Daniel Radcliffe gets ¬£20M fortune as he turns 18 Monday .\nYoung actor says he has no plans to fritter his cash away .\nRadcliffe's earnings from first five Potter films have been held in trust fund .\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Step 3: Use full dataset for comprehensive training","metadata":{}},{"cell_type":"code","source":"print(f\"\\nüî• Using FULL dataset for comprehensive training...\")\n\n# Using complete dataset for best results\ntrain_data = dataset['train']      # Full training set: ~287k examples\nval_data = dataset['validation']   # Full validation set: ~13k examples  \ntest_data = dataset['test']        # Full test set: ~11k examples\n\nprint(f\"   üìä Full dataset sizes:\")\nprint(f\"   - Training: {len(train_data):,} examples\")\nprint(f\"   - Validation: {len(val_data):,} examples\")\nprint(f\"   - Testing: {len(test_data):,} examples\")\nprint(f\"   ‚ö†Ô∏è  This will take significantly longer to train!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T13:56:59.245764Z","iopub.execute_input":"2025-08-19T13:56:59.245972Z","iopub.status.idle":"2025-08-19T13:56:59.789055Z","shell.execute_reply.started":"2025-08-19T13:56:59.245952Z","shell.execute_reply":"2025-08-19T13:56:59.782603Z"}},"outputs":[{"name":"stdout","text":"\nüî• Using FULL dataset for comprehensive training...\n   üìä Full dataset sizes:\n   - Training: 287,113 examples\n   - Validation: 13,368 examples\n   - Testing: 11,490 examples\n   ‚ö†Ô∏è  This will take significantly longer to train!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Step 4: Load model \n","metadata":{}},{"cell_type":"code","source":"model_name = \"facebook/bart-base\"  # Good balance of quality and efficiency\n# Alternative options:\n# model_name = \"t5-small\"  # Faster, less memory  \n# model_name = \"facebook/bart-large-cnn\"  # Best quality, more memory\n\nprint(f\"   Model: {model_name}\")\n\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n    model = model.to(device)\n    print(f\"‚úÖ Model loaded successfully!\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading model: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T13:56:59.791423Z","iopub.execute_input":"2025-08-19T13:56:59.791739Z","iopub.status.idle":"2025-08-19T13:57:04.129317Z","shell.execute_reply.started":"2025-08-19T13:56:59.791713Z","shell.execute_reply":"2025-08-19T13:57:04.124127Z"}},"outputs":[{"name":"stdout","text":"   Model: facebook/bart-base\n‚úÖ Model loaded successfully!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Step 5: Preprocessing function ","metadata":{}},{"cell_type":"code","source":"def preprocess_batch(examples):\n    \"\"\"Preprocess data efficiently for Kaggle\"\"\"\n    # BART doesn't need task prefix (unlike T5)\n    articles = examples['article']\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        articles,\n        max_length=1024,               # BART can handle longer sequences\n        truncation=True,\n        padding='max_length',\n        return_tensors='pt'\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples['highlights'],\n            max_length=142,            # Slightly longer for BART\n            truncation=True,\n            padding='max_length',\n            return_tensors='pt'\n        )\n    \n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T13:57:04.131140Z","iopub.execute_input":"2025-08-19T13:57:04.131353Z","iopub.status.idle":"2025-08-19T13:57:04.143318Z","shell.execute_reply.started":"2025-08-19T13:57:04.131331Z","shell.execute_reply":"2025-08-19T13:57:04.137170Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Step 6: Apply preprocessing\n","metadata":{}},{"cell_type":"code","source":"print(f\"\\nüîß Preprocessing data...\")\n\ntrain_dataset = train_data.map(\n    preprocess_batch,\n    batched=True,\n    remove_columns=['article', 'highlights', 'id'],\n    desc=\"Processing training data\"\n)\n\nval_dataset = val_data.map(\n    preprocess_batch,\n    batched=True,\n    remove_columns=['article', 'highlights', 'id'],\n    desc=\"Processing validation data\"\n)\n\ntest_dataset = test_data.map(\n    preprocess_batch,\n    batched=True,\n    remove_columns=['article', 'highlights', 'id'],\n    desc=\"Processing test data\"\n)\n\nprint(f\"‚úÖ Preprocessing complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T13:57:04.145200Z","iopub.execute_input":"2025-08-19T13:57:04.145415Z","iopub.status.idle":"2025-08-19T14:02:27.799376Z","shell.execute_reply.started":"2025-08-19T13:57:04.145394Z","shell.execute_reply":"2025-08-19T14:02:27.793105Z"}},"outputs":[{"name":"stdout","text":"\nüîß Preprocessing data...\n","output_type":"stream"},{"name":"stderr","text":"Processing training data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 287113/287113 [04:53<00:00, 979.00 examples/s] \nProcessing validation data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13368/13368 [00:13<00:00, 1008.72 examples/s]\nProcessing test data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11490/11490 [00:12<00:00, 948.36 examples/s]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Preprocessing complete!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Step 7: ROUGE evaluation function\n\n","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    \"\"\"Compute ROUGE scores\"\"\"\n    try:\n        rouge = evaluate.load('rouge')\n        predictions, labels = eval_pred\n        \n        # Decode predictions and labels\n        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        \n        # Clean up text\n        decoded_preds = [pred.strip() for pred in decoded_preds]\n        decoded_labels = [label.strip() for label in decoded_labels]\n        \n        result = rouge.compute(\n            predictions=decoded_preds,\n            references=decoded_labels,\n            use_stemmer=True\n        )\n        \n        return {\n            'rouge1': result['rouge1'],\n            'rouge2': result['rouge2'],\n            'rougeL': result['rougeL']\n        }\n    except Exception as e:\n        print(f\"Warning: Could not compute metrics: {e}\")\n        return {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:02:27.802074Z","iopub.execute_input":"2025-08-19T14:02:27.802414Z","iopub.status.idle":"2025-08-19T14:02:27.815949Z","shell.execute_reply.started":"2025-08-19T14:02:27.802388Z","shell.execute_reply":"2025-08-19T14:02:27.811084Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Step 8: Training arguments optimized for full dataset\n","metadata":{}},{"cell_type":"code","source":"print(f\"\\n‚öôÔ∏è Setting up training for FULL dataset...\")\n\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/results',           \n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    weight_decay=0.01,\n    learning_rate=2e-5,\n    logging_dir='/kaggle/working/logs',\n    logging_steps=100,\n    logging_strategy=\"steps\",\n    eval_strategy=\"steps\",                # <-- use this\n    eval_steps=5000,\n    save_steps=10000,\n    save_strategy=\"steps\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"rouge1\",\n    greater_is_better=True,\n    fp16=torch.cuda.is_available(),\n    dataloader_num_workers=0,\n    remove_unused_columns=False,\n    report_to=None,\n    push_to_hub=False,\n    max_steps=50000,\n    dataloader_pin_memory=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:02:27.817836Z","iopub.execute_input":"2025-08-19T14:02:27.818093Z","iopub.status.idle":"2025-08-19T14:02:33.594445Z","shell.execute_reply.started":"2025-08-19T14:02:27.818069Z","shell.execute_reply":"2025-08-19T14:02:33.589546Z"}},"outputs":[{"name":"stdout","text":"\n‚öôÔ∏è Setting up training for FULL dataset...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1755612149.931300      10 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:232\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Step 9: Data collator\n","metadata":{}},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model, \n    padding=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:02:33.597080Z","iopub.execute_input":"2025-08-19T14:02:33.597369Z","iopub.status.idle":"2025-08-19T14:02:33.605416Z","shell.execute_reply.started":"2025-08-19T14:02:33.597342Z","shell.execute_reply":"2025-08-19T14:02:33.601818Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Step 10: Create trainer\n","metadata":{}},{"cell_type":"code","source":"print(f\"üèÉ‚Äç‚ôÇÔ∏è Creating trainer...\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:02:33.608866Z","iopub.execute_input":"2025-08-19T14:02:33.609168Z","iopub.status.idle":"2025-08-19T14:02:34.141926Z","shell.execute_reply.started":"2025-08-19T14:02:33.609140Z","shell.execute_reply":"2025-08-19T14:02:34.133958Z"}},"outputs":[{"name":"stdout","text":"üèÉ‚Äç‚ôÇÔ∏è Creating trainer...\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(f\"Model is on: {next(model.parameters()).device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:02:34.143605Z","iopub.execute_input":"2025-08-19T14:02:34.143860Z","iopub.status.idle":"2025-08-19T14:02:34.154160Z","shell.execute_reply.started":"2025-08-19T14:02:34.143828Z","shell.execute_reply":"2025-08-19T14:02:34.149094Z"}},"outputs":[{"name":"stdout","text":"Model is on: xla:0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Step 11: Training with progress tracking\n","metadata":{}},{"cell_type":"code","source":"print(f\"\\nüéØ Starting training...\")\nprint(f\"   üìä Training info:\")\nprint(f\"   - Examples: {len(train_dataset):,}\")\nprint(f\"   - Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   - Epochs: {training_args.num_train_epochs}\")\nprint(f\"   - Expected steps: {len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs}\")\n\ntry:\n    trainer.train()\n    print(f\"‚úÖ Training completed successfully!\")\nexcept Exception as e:\n    print(f\"‚ùå Training error: {e}\")\n    print(\"üí° Try reducing batch size or dataset size\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:02:34.155893Z","iopub.execute_input":"2025-08-19T14:02:34.156115Z","execution_failed":"2025-08-19T14:04:15.711Z"}},"outputs":[{"name":"stdout","text":"\nüéØ Starting training...\n   üìä Training info:\n   - Examples: 287,113\n   - Batch size: 2\n   - Epochs: 3\n   - Expected steps: 430668\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/50000 : < :, Epoch 0.00/12]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# Step 12: Evaluate on test set\n","metadata":{}},{"cell_type":"code","source":"print(f\"\\nüìä Evaluating model...\")\ntry:\n    test_results = trainer.evaluate(eval_dataset=test_dataset)\n    print(f\"üéØ Test Results:\")\n    for key, value in test_results.items():\n        if 'rouge' in key.lower():\n            print(f\"   {key}: {value:.4f}\")\nexcept Exception as e:\n    print(f\"‚ùå Evaluation error: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 13: Generation function\n","metadata":{}},{"cell_type":"code","source":"def generate_summary(text, max_length=120):\n    \"\"\"Generate summary with BART model\"\"\"\n    try:\n        # BART doesn't need task prefix (unlike T5)\n        inputs = tokenizer.encode(\n            text, \n            return_tensors='pt', \n            max_length=1024,           # BART can handle longer inputs\n            truncation=True\n        ).to(device)\n        \n        with torch.no_grad():\n            summary_ids = model.generate(\n                inputs,\n                max_length=max_length,\n                min_length=30,\n                length_penalty=2.0,\n                num_beams=4,\n                early_stopping=True,\n                no_repeat_ngram_size=3      # Prevent repetition\n            )\n        \n        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n        return summary.strip()\n    \n    except Exception as e:\n        return f\"Error generating summary: {e}\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 14: Test on examples\n","metadata":{}},{"cell_type":"code","source":"print(f\"\\nüß™ Testing on sample articles...\")\nprint(f\"=\"*70)\n\nsample_texts = [\n    dataset['test'][0]['article'] if 'dataset' in locals() else train_dataset[0],\n    dataset['test'][1]['article'] if 'dataset' in locals() else train_dataset[1],\n    dataset['test'][2]['article'] if 'dataset' in locals() else train_dataset[2]\n]\n\nfor i in range(min(3, len(sample_texts))):\n    print(f\"\\nüì∞ Example {i+1}:\")\n    print(f\"-\" * 50)\n    \n    # Get test example if available\n    if i < len(test_dataset):\n        # Reconstruct text from tokens (approximate)\n        article_text = \"Sample article for testing summarization...\"\n    else:\n        article_text = \"Sample text for demonstration.\"\n    \n    summary = generate_summary(article_text)\n    \n    print(f\"Input: {article_text[:200]}...\")\n    print(f\"Summary: {summary}\")\n    print(f\"-\" * 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 15: Save model to Kaggle output\n","metadata":{}},{"cell_type":"code","source":"print(f\"\\nüíæ Saving model...\")\noutput_dir = '/kaggle/working/summarization_model'\n\ntry:\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"‚úÖ Model saved to {output_dir}\")\n    \n    # Create a simple usage guide\n    with open('/kaggle/working/how_to_use.txt', 'w') as f:\n        f.write(\"How to use your trained summarization model:\\n\\n\")\n        f.write(\"1. Load the model:\\n\")\n        f.write(\"   from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\n\")\n        f.write(f\"   tokenizer = AutoTokenizer.from_pretrained('{output_dir}')\\n\")\n        f.write(f\"   model = AutoModelForSeq2SeqLM.from_pretrained('{output_dir}')\\n\\n\")\n        f.write(\"2. Generate summary:\\n\")\n        f.write(\"   # Use the generate_summary function from this notebook\\n\")\n        f.write(\"   summary = generate_summary('Your article text here')\\n\")\n    \n    print(f\"üìù Usage guide saved to /kaggle/working/how_to_use.txt\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Error saving model: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 16: Create results summary for full dataset\n","metadata":{}},{"cell_type":"code","source":"print(f\"\\nüìã Creating results summary...\")\nresults_df = pd.DataFrame({\n    'Metric': ['Training Examples', 'Validation Examples', 'Test Examples', 'Model Used', 'Training Epochs', 'Device Used', 'Max Steps'],\n    'Value': [f\"{len(train_data):,}\", f\"{len(val_data):,}\", f\"{len(test_data):,}\", model_name, training_args.num_train_epochs, str(device), training_args.max_steps]\n})\n\n# Save results\nresults_df.to_csv('/kaggle/working/training_summary.csv', index=False)\nprint(f\"‚úÖ Results saved to /kaggle/working/training_summary.csv\")\n\n# Final memory cleanup\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(f\"\\nüéâ All done! Your summarization model is ready!\")\nprint(f\"üìÅ Files created:\")\nprint(f\"   - Model: /kaggle/working/summarization_model/\")\nprint(f\"   - Usage guide: /kaggle/working/how_to_use.txt\")\nprint(f\"   - Training log: /kaggle/working/training_summary.csv\")\n\n# Quick demo\ndemo_text = \"\"\"\nScientists have discovered a new species of dinosaur in Argentina. The massive creature, \nnamed Meraxes gigas, lived about 90 million years ago and had tiny arms similar to \nTyrannosaurus rex. Despite having small arms, the dinosaur was a fearsome predator \nthat could grow up to 36 feet long and weigh over 4 tons. This discovery helps \nresearchers better understand how different dinosaur species evolved similar features.\n\"\"\"\n\nprint(f\"\\nüîç Quick demo:\")\nprint(f\"Original: {demo_text}\")\nprint(f\"Summary: {generate_summary(demo_text)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}